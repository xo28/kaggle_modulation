{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import h5py\nimport numpy as np\nimport os,random\nfrom keras.layers import Input,Reshape,ZeroPadding2D,Conv2D,Dropout,Flatten,Dense,Activation,MaxPooling2D,AlphaDropout,Conv1D,MaxPooling1D\nfrom keras import layers\nimport keras.models as Model\nfrom keras.regularizers import *\nfrom keras.optimizers import adam\nimport seaborn as sns\nimport keras\nimport keras.backend.tensorflow_backend as tfback\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport gc\n%matplotlib inline\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def _get_available_gpus():\n    \"\"\"Get a list of available gpu devices (formatted as strings).\n\n    # Returns\n        A list of available GPU devices.\n    \"\"\"\n    #global _LOCAL_DEVICES\n    if tfback._LOCAL_DEVICES is None:\n        devices = tf.config.list_logical_devices()\n        tfback._LOCAL_DEVICES = [x.name for x in devices]\n    return [x for x in tfback._LOCAL_DEVICES if 'device:gpu' in x.lower()]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfback._get_available_gpus = _get_available_gpus\ntfback._get_available_gpus()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tf.config.experimental_list_devices()\ntf.config.list_logical_devices()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading data\nf = h5py.File(\"../input/dataset/data.hdf5\", 'r')\nX_train = np.array(f['train'])\n# X_train = np.vstack((X_train, X_big_train))\n# X_train = torch.from_numpy(X_train).reshape(X_train.shape[0], 1,X_train.shape[1], X_train.shape[2])\n# X_train = X_train.reshape(X_train.shape[0],X_train.shape[2], X_train.shape[1])\nprint(X_train.shape)\n\nX_test = np.array(f['test'])\n# X_test = torch.from_numpy(X_test).reshape(X_test.shape[0], 1,X_test.shape[1], X_test.shape[2])\n\nY_raw_train = pd.read_csv('../input/dataset/train_labels.csv').to_numpy()\n# Y_train = np.vstack((Y_train, Y_big_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_index = {\n 'FM': 0,\n 'OQPSK':1,\n 'BPSK':2,\n '8PSK':3,\n 'AM-SSB-SC':4,\n '4ASK':5,\n '16PSK':6,\n 'AM-DSB-SC':7, \n 'QPSK': 8, \n 'OOK':9,\n    3:0,\n    6:1,\n    8:2,\n    9:3,\n    10:4,\n    11:5,\n    12:6,\n    16:7,\n    19:8,\n    22:9\n}\nindex_class = {\n 0:'FM',\n 1:'OQPSK',\n 2:'BPSK',\n 3:'8PSK',\n 4:'AM-SSB-SC',\n 5:'4ASK',\n 6:'16PSK',\n 7:'AM-DSB-SC', \n 8:'QPSK', \n 9:'OOK',\n}\n\nclassnum = 10\ndef classToIndex(catg):\n    return class_index[catg]\n\n#direct mapping, not one-hot\ndef setToNum(dataset):\n    output = np.zeros((dataset.shape[0], 1),dtype = int)\n    for li, catg in enumerate(dataset[:,1]):\n        output[li] = classToIndex(catg)\n    return output        \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_number = setToNum(Y_raw_train)\nprint(Y_number.shape)\nprint(Y_number.dtype)\n\nY_train = keras.utils.to_categorical(Y_number, num_classes=classnum, dtype='int64')\nprint(Y_train.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\"\"\"建立模型\"\"\"\nclasses = [\n#  '32PSK',\n#  '16APSK',\n#  '32QAM',\n 'FM',\n#  'GMSK',\n#  '32APSK',\n 'OQPSK',\n#  '8ASK',\n 'BPSK',\n '8PSK',\n 'AM-SSB-SC',\n '4ASK',\n '16PSK',\n#  '64APSK',\n#  '128QAM',\n#  '128APSK',\n 'AM-DSB-SC',\n#  'AM-SSB-WC',\n#  '64QAM',\n 'QPSK',\n#  '256QAM',\n#  'AM-DSB-WC',\n 'OOK']\n#  '16QAM']\ndata_format = 'channels_first'\n\ndef residual_stack(Xm,kennel_size,Seq,pool_size):\n    #1*1 Conv Linear\n    Xm = Conv2D(32, (1,1), padding='same', name=Seq+\"_conv1\", kernel_initializer='glorot_normal',data_format=data_format)(Xm)\n    #Residual Unit 1\n    Xm_shortcut = Xm\n    Xm = Conv2D(32, kennel_size, padding='same',activation=\"relu\",name=Seq+\"_conv2\", kernel_initializer='glorot_normal',data_format=data_format)(Xm)\n    Xm = Conv2D(32, kennel_size, padding='same', name=Seq+\"_conv3\", kernel_initializer='glorot_normal',data_format=data_format)(Xm)\n    Xm = layers.add([Xm,Xm_shortcut])\n    Xm = Activation(\"relu\")(Xm)\n    #Residual Unit 2\n    Xm_shortcut = Xm\n    Xm = Conv2D(32, kennel_size, padding='same',activation=\"relu\",name=Seq+\"_conv4\", kernel_initializer='glorot_normal',data_format=data_format)(Xm)\n    X = Conv2D(32, kennel_size, padding='same', name=Seq+\"_conv5\", kernel_initializer='glorot_normal',data_format=data_format)(Xm)\n    Xm = layers.add([Xm,Xm_shortcut])\n    Xm = Activation(\"relu\")(Xm)\n    #MaxPooling\n    Xm = MaxPooling2D(pool_size=pool_size, strides=pool_size, padding='valid', data_format=data_format)(Xm)\n    return Xm\n\n\nin_shp = (1024,2)#X_train.shape[1:]   #每个样本的维度[1024,2]\n#input layer\nXm_input = Input(in_shp)\nXm = Reshape([1,1024,2], input_shape=in_shp)(Xm_input)\n#Residual Srack\nXm = residual_stack(Xm,kennel_size=(3,3),Seq=\"ReStk0\",pool_size=(2,2))   #shape:(512,1,32)\nXm = residual_stack(Xm,kennel_size=(3,2),Seq=\"ReStk1\",pool_size=(2,1))   #shape:(256,1,32)\nXm = residual_stack(Xm,kennel_size=(3,2),Seq=\"ReStk2\",pool_size=(2,1))   #shape:(128,1,32)\nXm = residual_stack(Xm,kennel_size=(3,2),Seq=\"ReStk3\",pool_size=(2,1))   #shape:(64,1,32)\nXm = residual_stack(Xm,kennel_size=(3,2),Seq=\"ReStk4\",pool_size=(2,1))   #shape:(32,1,32)\nXm = residual_stack(Xm,kennel_size=(3,2),Seq=\"ReStk5\",pool_size=(2,1))   #shape:(16,1,32)\n\n#############################################################################\n#      多次尝试发现减少一层全连接层能使loss下降更快\n#      将AlphaDropout设置为0.3似乎比0.5效果更好\n#############################################################################\n#Full Con 1\nXm = Flatten(data_format=data_format)(Xm)\nXm = Dense(128, activation='selu', kernel_initializer='glorot_normal', name=\"dense1\")(Xm)\nXm = AlphaDropout(0.3)(Xm)\n#Full Con 2\n# Xm = Flatten(data_format=data_format)(Xm)\n# Xm = Dense(128, activation='selu', kernel_initializer='glorot_normal', name=\"dense2\")(Xm)\n# Xm = AlphaDropout(0.5)(Xm)\n#Full Con 2\nXm = Dense(len(classes), kernel_initializer='glorot_normal', name=\"dense3\")(Xm)\n#SoftMax\nXm = Activation('softmax')(Xm)\n#Create Model\nmodel = Model.Model(inputs=Xm_input,outputs=Xm)\nadam = keras.optimizers.Adam(lr=0.001)\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=[keras.metrics.categorical_accuracy])\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"训练模型\"\"\"\n#############################################################################\n#      当val_loss连续10次迭代不再减小或总迭代次数大于100时停止\n#      将最小验证损失的模型保存\n#############################################################################\nprint(tf.test.gpu_device_name())\n# filepath = 'drive/RadioModulationRecognition/Models/ResNet_Model_72w.h5'\nhistory = model.fit(X_train,\n    Y_train,\n    batch_size=1024,\n    epochs=1000,\n#     verbose=2,\n#     validation_data=(X_test, Y_test),\n    #validation_split = 0.3,\n                    \n#     callbacks = [\n#         keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True, mode='auto'),\n#         keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n#     ]\n)\n\n# we re-load the best weights once training is finished\n# model.load_weights(filepath)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_list = history.history['loss']\naccuracy_list = history.history['categorical_accuracy']\nplt.plot(range(len(loss_list)),loss_list)\nplt.plot(range(len(accuracy_list)),accuracy_list)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index_class = {\n 0:'FM',\n 1:'OQPSK',\n 2:'BPSK',\n 3:'8PSK',\n 4:'AM-SSB-SC',\n 5:'4ASK',\n 6:'16PSK',\n 7:'AM-DSB-SC', \n 8:'QPSK', \n 9:'OOK',\n}\n\n# class to name\ndef classToName(cls):\n    return index_class[cls]\n\ndef onehotToNum(dataset):\n    output = np.array(np.argmax(dataset, axis = 1), ndmin = 2).T\n    return output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_Y_hat = model.predict(X_train, batch_size=1024)\ntrainout = onehotToNum(train_Y_hat)\ntotal = 30000\ntraincorrect = 0\ntraincorrect += (trainout == Y_number).sum().item()\n\nprint(train_Y_hat.shape)\nprint('Train Accuracy of the model over the 30000 train modulations: {} %'.format(100 * traincorrect / total)) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_Y_hat = model.predict(X_test, batch_size=1024)\ntestout = onehotToNum(test_Y_hat)\n\npredictionlist = testout.tolist()\nnames = []\n\nfor line in predictionlist:\n    names.append(classToName(line[0]))\n    \noutputs = np.asarray(names, dtype = str)\nprint(outputs)\ncategories = pd.DataFrame(outputs, columns = ['Category'])       \ncategories.to_csv('categories.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}